---
layout:     post
title:      CBAM
subtitle:   CBAM Convolutional Block Attention Module
date:       2020-04-07
author:     LZY
header-img: img/xiaohuangren.jpg
catalog: true
tags:
    - 深度学习
---

# 注意力机制

## 通道注意力

![](/img/202006021.png)

如上图，对spatial维度分别进行max-pooling 和 avg-pooling，得到两个一维向量。

在SeNet的基础上增加了max-pooling进行补充

## 空间注意力

![](/img/202006023.png)

如上图，对通道C分别进行max-pooling 和 avg-pooling，得到了两个二维的feature，将其按通道维度拼接在一起得到一个通道数为2的feature map

## 添加到Resnet中

![](/img/20200600.png)

[Github](https://github.com/Jongchan/attention-module)

## MMdetection

创建文件 `/mmdetection/mmdet/models/backbones/bam.py` (from Github above)

在文件`/mmdetection/mmdet/models/backbones/resnet.py`添加

```python

@BACKBONES.register_module()
class ResNetBam(ResNet):

    def __init__(self, **kwargs):
        super(ResNetBam, self).__init__(**kwargs)
        self.bam1 = BAM(64*self.block.expansion)
        self.bam2 = BAM(128*self.block.expansion)
        self.bam3 = BAM(256*self.block.expansion)
        self.bam4 = BAM(512*self.block.expansion)

    def forward(self, x):
        if self.deep_stem:
            x = self.stem(x)
        else:
            x = self.conv1(x)
            x = self.norm1(x)
            x = self.relu(x)
        x = self.maxpool(x)
        outs = []
        for i, layer_name in enumerate(self.res_layers):
            res_layer = getattr(self, layer_name)
            x = res_layer(x)

            # CBAM: Convolutional Block Attention Module
            bam = getattr(self, f'bam{i+1}')
            x = bam(x)

            if i in self.out_indices:
                outs.append(x)
        return tuple(outs)
```

在`/mmdetection/mmdet/models/backbones/__init__.py`中导入ResNetBam类
