---
layout:     post
title:      Pytorch 常用的学习率设置方式 
subtitle:   自定义学习率
date:       2019-12-25
author:     LZY
header-img: img/whatisnext.jpg
catalog: true
tags:
    - 学习率
    - Pytorch
---

# 自定义学习率

```python
def adjust_learning_rate(optimizer, epoch, lr):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    lr = lr * (0.1 ** (epoch // 30))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
```

# LambdaLR自定义学习率调整方式


```python
torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)

# optimizer：优化器
# lr_lambda：自定义的何时更新学习率的lambda表达式，可以是一个lambda也可以是一个lambda表达式的列表，当参数为lambda的列表是需要保证optimizer所包含的group数量和该list长度相同否则会报错。
# last_epoch：最后一个更新学习率的epoch，默认为-1，一直更新

```

# StepLR分阶段更新学习率


```python
torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)

# step_size：每多少个epoch更新一次学习率
# gamma：学习率decay因子

```

# MultiStepLR顶点分阶段更新学习率


```python
torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)

# milestones：需要更新学习率的epoch的列表，需要列表内的值是递增的
# milestones = [1,10,20,...]

```

# 以较大学习率微调全连接层，较小学习率微调卷积层

```python
model = torchvision.models.resnet18(pretrained=True)
finetuned_parameters = list(map(id, model.fc.parameters()))
conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)
parameters = [{'params': conv_parameters, 'lr': 1e-3}, 
              {'params': model.fc.parameters()}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
```


