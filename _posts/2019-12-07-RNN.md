---
layout:     post
title:      RNN
subtitle:   递归神经网络
date:       2019-12-07
author:     LZY
header-img: img/whatisnext.jpg
catalog: true
tags:
    - 深度学习
    - 神经网络
    - 递归神经网络
---

# 递归神经网络

> 循环神经网络（Recurrent Neural Network，RNN）是一种处理序列变化数据的神经网络。

传统的神经网络，也称为前馈神经网络。它有输入层，隐藏层和输出层。

![](/img/201912229586.png)

在神经网络中添加可以传递先前信息的循环，便是递归神经网络。

![](/img/201912226582002.png)

1. 初始化网络层和初始隐藏状态。隐藏状态的形状和维度将取决于递归神经网络的形状和维度。

2. 循环输入，将单词和隐藏状态传递给RNN。RNN返回输出和修改的隐藏状态，继续循环。

3. 将输出传递给前馈层，然后返回预测。

# 短期记忆

![](/img/25092019121222.png)

短期记忆问题是由梯度消失问题引起的。梯度是用于调整网络内部权重的值从而更新整个网络。在进行反向传播时，图层中的每个节点都会根据渐变效果计算它在其前面的图层中的渐变。因此，如果在它之前对层的调整很小，那么对当前层的调整将更小。这会导致渐变在向后传播时呈指数级收缩。由于梯度极小，内部权重几乎没有调整，因此较早的层无法进行任何学习。这就是消失的梯度问题。

![](/img/20191222698662.png)

由于梯度消失，RNN会受到短期记忆的影响,不会跨时间步骤学习远程依赖性。

# LSTM / GRU

## LSTM

> 通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息

相比RNN只有一个传递状态 ht ，LSTM有两个传输状态，一个 ct (cell state)，和一个ht (hidden state)。

Tips：RNN中的 ht 对于LSTM中的ct

其中对于传递下去的 ct 改变得很慢，通常输出的ct是上一个状态传过来的c(t-1)加上一些数值。

而ht则在不同节点下往往会有很大的区别。

![](/img/20191222598633022.jpg)

- 忘记阶段

- 选择记忆阶段

- 输出阶段

![](/img/2019122236699.jpg)

LSTMh相对RNN参数变多，也使得训练难度加大。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型

## GRU

GRU通过上一个传输下来的状态h(t-1)和当前节点的输入 xt来获取两个门控状态

r控制重置的门控reset gate

z为控制更新的门控update gate

![](/img/2019121b8622.jpg)

![](/img/2019121222633.jpg)

---

与LSTM相比，GRU内部少了一个门控，参数比LSTM少，但是却也能够达到与LSTM相当的功能。考虑到硬件的计算能力和时间成本，因而很多时候选择更加实用的GRU
