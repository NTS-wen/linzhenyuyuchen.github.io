---
layout:     post
title:      经典卷积神经网络模型
subtitle:   笔记 AlexNet VGGNet Inception ResNet
date:       2019-09-02
author:     LZY
header-img: img/Netural.jpg
catalog: true
tags:
    - 卷积神经网络
    - AlexNet
    - VGGNet
    - Inception
    - ResNet
---
# 前言
> 实现不涉及实际数据的训练，建立一个完整的CNN对它的每个batch的前馈计算和反馈计算的速度进行测试


# AlexNet

>Alexnet 拥有5个卷积层，其中3个卷积层后面连接了最大池化层，最后还有3个全连接层。

1. 成功使用ReLU作为激活函数，并验证在层数较深的CNN中效果比Sigmoid好，解决了Sigmoid在网络较深时的梯度弥散问题

2. 全连接层使用Dropout，随机忽略部分神经元，缓解模型过拟合

3. 全部使用最大池化，步长比池化核尺寸小，使得池化层输出之间有重叠和覆盖，提升了特征的丰富性，避免平均池化的模糊化效果

4. 提出LRN层，对局部神经元的活动创造竞争机制，使得响应较大的值变得相对更大，抑制其它反馈小的神经元，增强模型的泛化性

5. 数据增强

6. 对图片的RGB数据进行PCA处理，并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，降低错误率


---

# VGGNet

>通过反复堆叠3x3小型卷积核和2x2最大池化层，VGGNet成功地构筑了16~19层深的卷积神经网络

### VGG 16 / 网络结构 D

![VGG STRUCTURE](/img/VGG-STRUCTURE.png)

1、输入224x224x3的图片，经64个3x3的卷积核作两次卷积+ReLU，卷积后的尺寸变为224x224x64

2、作max pooling（最大化池化），池化单元尺寸为2x2（效果为图像尺寸减半），池化后的尺寸变为112x112x64

3、经128个3x3的卷积核作两次卷积+ReLU，尺寸变为112x112x128

4、作2x2的max pooling池化，尺寸变为56x56x128

5、经256个3x3的卷积核作三次卷积+ReLU，尺寸变为56x56x256

6、作2x2的max pooling池化，尺寸变为28x28x256

7、经512个3x3的卷积核作三次卷积+ReLU，尺寸变为28x28x512

8、作2x2的max pooling池化，尺寸变为14x14x512

9、经512个3x3的卷积核作三次卷积+ReLU，尺寸变为14x14x512

10、作2x2的max pooling池化，尺寸变为7x7x512

11、与两层1x1x4096，一层1x1x1000进行全连接+ReLU（共三层）

12、通过softmax输出1000个预测结果

### VGG 网络结构

![VGG CONFIGURATION](/img/VGG-Configuration.png)

### Feature

1. 全部使用3x3小型卷积核和2x2最大池化层，通过不断加深网络提升性能

2. 参数量主要消耗在后面的3个全连接层

3. C比B多了几个1x1的卷积层，但是输入输出通道数不变，没有发生降维，主要是进行线性变换

4. 5段卷积 每段尾部连接一个最大池化层用来缩小图片尺寸

5. 2个3x3卷积层相当于1个5x5卷积层，但参数只有其(2x3x3)/(1x5x5) 同理3个3x3卷积层相当于1个7x7卷积层而参数只有其(3x3x3)/(1x7x7) 且多个卷积层拥有更多的非线性变换（使用多次ReLU激活函数），使得对CNN对特征的学习能力更强

6. LRN层作用不大

7. 层数越多效果越好

8. 1x1的卷积层也是很有效的，但没有3x3的卷积层好，大一点的卷积核可以学习更大的空间特征

---

# Google Inception Net V1

>Inception Net 的主要目标是找到最优的稀疏结构单元(Inception Module)

1. 使用全局平均池化，即将图片尺寸变为1x1，来取代全连接层

2. Inception Module是在NIN(Network in Network)的级联卷积层和MIPConc层基础上增加了**分支网络**，用多个分支提取不同抽象程度的高阶特征，丰富网络的表达能力

3. MIPConv允许在输出通道之间组合信息，基本等效于普通卷积层后再连接1x1的卷积和ReLU激活函数

4. 1x1的卷积可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道进行升维和降维

5. Factorization into small convolutions很有效，可以降低参数量，减轻过拟合，增强网络的非线性的表达能力

6. 卷积网络从输入到输出，应该让图片尺寸逐渐变小，输出通道数逐渐增加，即空间结构简化，将空间信息转化为高阶抽象的特征信息

---

# ResNet

>ResNet 将输入x直接传到输出作为初始结果，学习的目标不再是期望输出H(x)，而是F(x)=H(x)-x，即输出和输入的差别（残差）

1. 与普通直连的卷积神经网络最大区别在于，ResNet有很多旁路支线将输入直接连到后面的层，使得后面的 层可以直接学习残差，这种结构可称为shortcut / skip connections

2. V2 和 V1 的主要区别在于，V2 前馈信号和反馈信号直接传输，skip connection 的非线性激活函数替换为identity mappings(y=x)，每一层都使用了Batch Normalization 这样处理之后，新的残差学习单元(residual unit)比V1更容易训练且泛化性更强

---

# ResNeXt

>ResNeXt 据说是解决目标识别问题的最先进技术。它建立在 inception 和 resnet 的概念上，并带来改进的新架构。下图是对 ResNeXt 模块中的残差模块的总结。

![](/img/resnext_error.png)

---

# DenseNet

>Densely Connected Convolutional Networks

[Reference](https://arxiv.org/pdf/1608.06993.pdf)

[Github](https://github.com/liuzhuang13/DenseNet)

**优点：**

1. 减轻了vanishing-gradient（梯度消失）
2. 加强了feature的传递
3. 更有效地利用了feature
4. 一定程度上较少了参数数量

DenseNet与传统神经网络不同的是，dense block每一层的输入来自于前面所有层的输出 "Each layer takes all preceding feature-maps as input"

![dense block](/img/densenet.jpg)

Dense block的每个卷积层的输出feature map数量都很小，每一层都直接连接input和loss，得益于这种设计，DenseNet可以更好地减轻梯度消失现象

![](/img/densenet-horse.jpg)

- a 1×1 convolution can be introduced as `bottleneck layer` before each 3×3 convolutionto  reduce  the  number  of  input  feature-maps,  and  thus  toimprove computational efficiency

- We use 1×1 convolution followed by 2×2 average pooling as `transitionlayers` between two contiguous dense blocks

---


# CNN 进化史

>CNN的起点是神经认知机模型，此时已经出现了卷积结构，经典的LeNet诞生于1998年。然而之后CNN的锋芒开始被SVM等模型盖过。随着ReLU、dropout的提出，以及GPU和大数据带来的历史机遇，CNN在2012年迎来了历史突破：AlexNet。随后几年，CNN呈现爆发式发展，各种CNN模型涌现出来。

Reference:[CNN 进化史](https://my.oschina.net/u/876354/blog/1797489)

![](/img/CNN-his.png)


### CNN的主要演进方向如下：

1. 网络结构加深
2. 加强卷积功能
3. 从分类到检测
4. 新增功能模块

### CNN经典模型

![CNN经典模型](/img/CNN-NET.png "CNN经典模型")

>CNN 层次越来越深,结构越来越复杂,模型效果越来越好

---

# Q & A

### What are hyperparameters in machine learning?

[reference](https://www.quora.com/What-are-hyperparameters-in-machine-learning)

- 在机器学习中，我们定义超参数来区别于模型的参数

- 模型的参数是待学习的参数

- 但超参数通常在实际训练开始前就固定了

**超参数列举：**


- Number of leaves or depth of a tree
- Number of latent factors in a matrix factorization
- Learning rate (in many models)
- Number of hidden layers in a deep neural network
- Number of clusters in a k-means clustering