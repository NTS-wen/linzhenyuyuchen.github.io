---
layout:     post
title:      目标检测
subtitle:   常见模型架构 评价指标 IoU mAP
date:       2019-11-15
author:     LZY
header-img: img/whatisnext.jpg
catalog: true
tags:
    - 目标检测
    - 深度学习
---

# 目标检测

目标检测定位图像中物体的位置，并在该物体周围绘制边界框，这通常涉及两个过程，分类物体类型，然后在该对象周围绘制一个框。

## 常见模型架构

- R-CNN
- Fast R-CNN
- Faster R-CNN
- Mask R-CNN
- SSD (Single Shot MultiBox Defender)
- YOLO (You Only Look Once)
- RetinaNet
- Objects as Points

### R-CNN 模型

>该技术结合了两种主要方法:使用一个高容量的卷积神经网络将候选区域(region-proposals)自底向上的传播，用来定位和分割目标；如果有标签的训练数据比较少，可以使用训练好的参数作为辅助，进行微调(fine tuning)，能够得到非常好的识别效果提升。

进行特定领域的微调，从而获得高性能的提升。由于将候选区域(region-proposals)与卷积神经网络相结合，论文的作者将该算法命名为R-CNN(Regions with CNN features)。

**该模型中的目标检测系统由三个模块组成:**

第一个负责生成类别无关的候选区域，这些区域定义了一个候选检测区域的集合。采用选择性搜索(selective search)方法来生成区域类别，根据颜色、纹理、形状和大小选择搜索对相似的区域进行分组。

第二个模块是一个大型卷积神经网络，负责从每个区域提取固定长度的特征向量。在特征提取方面，该模型使用CNN的一个Caffe实现版本对每个候选区域抽取一个4096维度的特征向量。将227×227 RGB图像通过5个卷积层和2个完全连接层进行前向传播，计算特征。

第三个模块由一个指定类别的支持向量机组成。

**缺点：**

1. 训练需要多阶段: 先用ConvNet进行微调，再用SVM进行分类，最后通过regression对 bounding box进行微调。

2. 训练空间和时间成本大: 因为像VGG16这样的深度网络占用了大量的空间。

3. 目标检测慢: 因为其需要对每个目标候选进行前向计算。

### Fast R-CNN

在其架构中， Fast R-CNN接收图像以及一组目标候选作为输入。然后通过卷积层和池化层对图像进行处理，生成卷积特征映射。然后，通过针对每个推荐区域，ROI池化层从每个特征映射中提取固定大小的特征向量。

然后将特征向量提供给全连接层。然后这些分支成两个输出层。其中一个为多个目标类生成softmax概率估计，而另一个为每个目标类生成4个实数值。这4个数字表示每个目标的边界框的位置。

### Faster R-CNN

Faster R-CNN模型由两个模块组成:负责提出区域的深度卷积网络和使用这些区域的Fast R-CNN探测器。候选区域网络(Region Proposal Network)以图像为输入，生成矩形目标候选的输出。每个矩形都有一个objectness score。

### Mask R-CNN

在该模型中，使用边界框和对每个像素点进行分类的语义分割对目标进行分类和定位。该模型通过在每个感兴趣区域(ROI)添加分割掩码(segmentation mask)的预测，扩展了Faster R-CNNR-CNN。Mask R-CNN产生两个输出:类标签和边界框。

### SSD

论文提出了一种利用单个深度神经网络对图像中目标进行预测的模型。该网络使用应用于特征映射的小卷积滤波器为每个目标类别生成分数。

这种方法使用了一个前馈卷积神经网络，针对那些方框里的目标类别实例，产生一个固定大小的边界框的集合和分数。增加了卷积特征层，允许多比例特征映射检测。

### You Only Look Once (YOLO)

论文提出了一种基于神经网络的图像边界框和类概率预测方法。

该模型的网络架构受到了用于图像分类的GoogLeNet模型的启发。该网络有24个卷积层和2个全连接层。该模型的主要挑战在于，它只能预测一个类，而且在鸟类等小目标上表现不佳。

### Objects as Points

论文提出将目标建模为单个点。它使用关键点估计来找到中心点，并回归到其他目标属性。

目标大小和姿态等属性是由图像中心位置的特征回归得到的。该模型将图像输入卷积神经网络，生成热力图。这些热力图中的峰值表示图像中目标的中心。为了估计人体姿态，该模型检测关节点（2D joint）位置，并在中心点位置对其进行回归。

## 非极大值抑制 (Non-Maximum Suppression,NMS)

检测器对于同一个目标会做出重复的检测。我们利用非极大值抑制来移除置信度低的重复检测。将预测按照置信度从高到低排列。如果任何预测和当前预测的类别相同并且两者 IoU 大于 0.5，我们就把它从这个序列中剔除。


## 评价指标

### IoU

1. ground-truth bounding boxes

2. 训练好的模型预测得到的bounding boxes

`IoU = 二者重叠区域 / 并集区域`


```python
def bb_intersection_over_union(boxA, boxB):
   # determine the (x, y)-coordinates of the intersection rectangle
   # 画个图会很明显，x左、y上取大的，x右、y下取小的，刚好对应交集
   xA = max(boxA[0], boxB[0])
   yA = max(boxA[1], boxB[1])
   xB = min(boxA[2], boxB[2])
   yB = min(boxA[3], boxB[3])
 ​
   # compute the area of intersection rectangle
   # 计算交集部分面积
   interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
 ​
   # compute the area of both the prediction and ground-truth rectangles
   # 计算预测值和真实值的面积
   boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
   boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
 ​
   # compute the intersection over union by taking the intersection
   # area and dividing it by the sum of prediction + ground-truth
   # areas - the interesection area
   # 计算IoU，即 交/（A+B-交）
   iou = interArea / float(boxAArea + boxBArea - interArea)
 ​
   # return the intersection over union value
   return iou
```

下面实现是batch的矩阵运算



```python
def bbox_overlaps(bboxes1, bboxes2, mode='iou'):
    """Calculate the ious between each bbox of bboxes1 and bboxes2.
    Args:
        bboxes1(ndarray): shape (n, 4)
        bboxes2(ndarray): shape (k, 4)
        mode(str): iou (intersection over union) or iof (intersection
            over foreground)
    Returns:
        ious(ndarray): shape (n, k)
    """

    assert mode in ['iou', 'iof']

    bboxes1 = bboxes1.astype(np.float32)
    bboxes2 = bboxes2.astype(np.float32)
    rows = bboxes1.shape[0]
    cols = bboxes2.shape[0]
    ious = np.zeros((rows, cols), dtype=np.float32)
    if rows * cols == 0:
        return ious
    exchange = False
    if bboxes1.shape[0] > bboxes2.shape[0]:
        bboxes1, bboxes2 = bboxes2, bboxes1
        ious = np.zeros((cols, rows), dtype=np.float32)
        exchange = True
    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (
        bboxes1[:, 3] - bboxes1[:, 1] + 1)
    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (
        bboxes2[:, 3] - bboxes2[:, 1] + 1)
    for i in range(bboxes1.shape[0]):
        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])
        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])
        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])
        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])
        overlap = np.maximum(x_end - x_start + 1, 0) * np.maximum(
            y_end - y_start + 1, 0)
        if mode == 'iou':
            union = area1[i] + area2 - overlap
        else:
            union = area1[i] if not exchange else area2
        ious[i, :] = overlap / union
    if exchange:
        ious = ious.T
    return ious
```

### mAP

mAP: mean Average Precision, 即各类别AP的平均值

- mAP: mean Average Precision, 即各类别AP的平均值

- AP: PR曲线下面积，其实是在0～1之间所有recall值的precision的平均值

- PR曲线: Precision-Recall曲线

- Precision: TP / (TP + FP)

- Recall: TP / (TP + FN)

- TP: IoU>Threshold的检测框数量（同一Ground Truth只计算一次）

- FP: IoU<=Threshold的检测框，或者是检测到同一个GT的多余检测框的数量

- FN: 没有检测到的GT的数量

#### VOC采用两种不同方法采样PR曲线

mAP计算示例(两种VOC方式比较)
https://zhuanlan.zhihu.com/spectre
https://zhuanlan.zhihu.com/p/60319755

