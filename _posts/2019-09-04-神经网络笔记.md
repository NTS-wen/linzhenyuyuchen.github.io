---
layout:     post
title:      神经网络笔记
subtitle:   记录零碎知识点
date:       2019-09-04
author:     LZY
header-img: img/lossfunction.jpeg
catalog: true
tags:
    - 神经网络
---

# 神经网络训练过程


### 一个神经网络的典型训练过程如下：

- 定义包含一些可学习参数（或者叫权重）的神经网络
- 在输入数据集上迭代
- 通过网络处理输入
- 计算损失（输出和正确答案的距离）
- 将梯度反向传播给网络的参数
- 更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient


# 神经网络实现流程

- 数据加载和数据处理 eg.通过torchvision加载CIFAR10里面的训练和测试数据集，并对数据进行标准化
- 定义卷积神经网络
- 定义损失函数和优化器
- 利用训练数据训练网络
- 利用测试数据测试网络

# 激活函数

Reference: [jianshu](https://www.jianshu.com/p/857d5859d2cc)

![Activation Function](/img/Activation-Function.png)

## Sigmoid

Sigmoid(x) = 1/(1+exp(-x))

（蓝色原函数，红色导函数）

![sigmoid](/img/sigmoid.png)

Sigmoid 可以将任何值转换为0~1概率，用于二分类

当使用Sigmoid 作为激活函数时，随着神经网络隐含层（hidden layer）层数的增加，训练误差反而加大。表现为：

- 靠近输出层的隐含层梯度较大，参数更新速度快，很快就会收敛

- 靠近输入层的隐含层梯度较小，参数更新速度慢，几乎和初始状态一样，随机分布

- 在含有四个隐藏层的网络结构中，第一层比第四层慢了接近100倍

这种现象就是梯度弥散（vanishing gradient），而另一种情况，梯度爆炸（exploding gradient），则是前面层的梯度，通过训练变大，导致后面层的梯度，以指数级增大

![sigmoid learning speed](/img/sigmoid-learning-speed.png)

由于Sigmoid的导数值小于1/4，x变化的速率要快于y变化的速率，随着层数的增加，连续不断执行sigmoid函数，就会导致，前面更新较大的幅度，后面更新较小的幅度，因此，网络在学习过程中，更倾向于更新后面（靠近输出层）的参数，而不是前面的参数（靠近输入层）

## tanh

tanh(x)= 2/(1+exp(-x*2))-1

（蓝色原函数，红色导函数）

![tanh](/img/tanh.png)

tanh，即双曲正切（hyperbolic tangent），类似于幅度增大sigmoid，将输入值转换为-1至1之间。tanh的导数取值范围在0至1之间，优于sigmoid的0至1/4，在一定程度上，减轻了梯度消失的问题。tanh的输出和输入能够保持非线性单调上升和下降关系，符合BP（back propagation）网络的梯度求解，容错性好，有界


### 二者对比

- sigmoid在输入处于[-1,1]之间时，函数值变化敏感，一旦接近或者超出区间就失去敏感性，处于饱和状态，影响神经网络预测的精度值

- tanh的变化敏感区间较宽，导数值渐进于0、1，符合人脑神经饱和的规律，比sigmoid函数延迟了饱和期

- tanh在原点附近与y=x函数形式相近，当激活值较低时，可以直接进行矩阵运算，训练相对容易

- tanh和sigmoid都是全部激活（fire），使得神经网络较重（heavy）


## ReLU
ReLU, 即Rectified Linear Unit，整流线性单元，激活部分神经元，增加稀疏性，当x小于0时，输出值为0，当x大于0时，输出值为x

y = max(0,x)

![ReLU](/img/relu.png)

### 二者对比

- Sigmoid 的导数，只有在0附近，具有较好的激活性，而在正负饱和区的梯度都接近于0，会造成梯度弥散；而relu的导数，在大于0时，梯度为常数，不会导致梯度弥散

- ReLU 函数在负半区的导数为0 ，当神经元激活值进入负半区，梯度就会为0，也就是说，这个神经元不会被训练，即稀疏性

- ReLU 函数的导数计算更快，程序实现就是一个if-else语句；而Sigmoid 函数要进行浮点四则运算，涉及到除法

### 三者对比

- tanh有正有负，可以投支持，反对，弃权。

- Sigmoid只有正，只能支持

- ReLU只能弃权和支持