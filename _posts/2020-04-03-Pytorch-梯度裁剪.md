---
layout:     post
title:      Pytorch 梯度裁剪
subtitle:   sudo
date:       2020-04-03
author:     LZY
header-img: img/xiaohuangren.jpg
catalog: true
tags:
    - Pytorch
---

# 梯度裁剪（Clipping Gradient）

应用于RNN中LSTM梯度消失

梯度裁剪原理：既然在BP过程中会产生梯度消失（就是偏导无限接近0，导致长时记忆无法更新），那么最简单粗暴的方法，设定阈值，当梯度小于阈值时，更新的梯度为阈值，解决梯度消失或爆炸的问题


```python
torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2)
```

```python
model = xxx
clip = 1
torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
```
