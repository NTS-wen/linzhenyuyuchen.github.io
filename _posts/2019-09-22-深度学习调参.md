---
layout:     post
title:      深度学习调参
subtitle:   调参技巧
date:       2019-09-22
author:     LZY
header-img: img/lossfunction.jpeg
catalog: true
tags:
    - 深度学习
---

# 好的实验环境是成功的一半

- 将各个参数的设定集中在一起，便于修改

- 输出模型的损失函数值及训练集和验证集上的准确率

- 设计子程序，根据给定的参数进行训练和监控并周期性保存评估结果，再由主程序分配参数及并行启动一系列子程序

# 画图

画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间

如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等

如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数

# 从粗到细分阶段调参

1. 参考相关论文

2. 按参数影响大小依次调整

3. 模型始终无法收敛，检查模型实现，数据等

# 提高速度

调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数

- 对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看

- 减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何

# 超参数范围

1. 学习率

2. 正则值

3. dropout值

建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果

不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)

# 经验参数

[Reference](https://cloud.tencent.com/developer/article/1367884)

1. learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。

不过更建议使用自适应梯度的办法，例如`adam,adadelta,rmsprop`等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率

对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题

2. 网络层数： 先从1层开始

3. 每层结点数： 16 32 128，超过1000的情况比较少见

4. `batch size`: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差

5. clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15

6. dropout： 0.5

7. L2正则：1.0，超过10的很少见

8. 词向量embedding大小：128，256

9. 正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用

10. 在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要

可选的常用值：1，2，4，8，16，【32】，64，128，256

推荐32比较常用。32、64、128、256都比较合适

256比较大，一次性计算的多，速度会快，但因为矩阵计算量较大，内存可能超过

小的mini batch size可能因为收敛的抖动比较厉害反而不容易卡在局部最低点 但是mini batch也不能太大，反而准确率下降

11. `epoch`

# 自动调参

1. Gird Search.这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合

2. Random Search.Bengio在[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练

3. Bayesian Optimization.[贝叶斯优化](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) 考虑到了不同参数对应的实验结果值，因此更节省时间

# Else

1. 激活函数

2. 优化算法

3. 损失函数

# 数据处理

**图像：**

- 数据增强

- Clip the most relevant value-space

