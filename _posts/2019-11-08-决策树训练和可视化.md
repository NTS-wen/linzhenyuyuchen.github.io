---
layout:     post
title:      决策树训练和可视化
subtitle:   Sklearn 决策树分类
date:       2019-11-08
author:     LZY
header-img: img/whatisnext.jpg
catalog: true
tags:
    - 决策树
    - Sklearn
---


# 决策树

> 白盒模型：分支节点,叶节点与分支条件,同时为了减少过拟合还有剪枝方法


## 超参数

在sklearn中我们可以用来提高决策树泛化能力的超参数主要有：

- max_depth:树的最大深度,也就是说当树的深度到达max_depth的时候无论还有多少可以分支的特征,决策树都会停止运算

- min_samples_split: 分裂所需的最小数量的节点数.当叶节点的样本数量小于该参数后,则不再生成分支.该分支的标签分类以该分支下标签最多的类别为准

- min_samples_leaf; 一个分支所需要的最少样本数,如果在分支之后,某一个新增叶节点的特征样本数小于该超参数,则退回,不再进行剪枝.退回后的叶节点的标签以该叶节点中最多的标签你为准

- min_weight_fraction_leaf: 最小的权重系数

- max_leaf_nodes:最大叶节点数,None时无限制,取整数时,忽略max_depth

## 分支方式

对于决策树而言,常见的决策树分支方式一共有三种,前两种是基于信息熵的,ID3(信息增益),C4.5(信息增益比),以及基于基尼系数的CART决策树

Sklearn中分支方式由超参数criterion决定:
- gini:默认参数,基于基尼系数
- entropy: 基于信息熵,也就是我们的ID3


# 可视化包安装

`pip install graphviz`

`pip install pydot`
