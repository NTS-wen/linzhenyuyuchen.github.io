---
layout:     post
title:      深度学习优化算法总结
subtitle:   优化方法的优缺点及其比较
date:       2019-09-01
author:     LZY
header-img: img/lossfunction.jpeg
catalog: true
tags:
    - 深度学习
    - 优化算法
    - 深度学习优化算法
---

#   前言

> 一些常见的优化方法进行直观介绍和简单的比较 SGD，Adagrad，Adadelta，Adam，Adamax，Nadam

Reference: [知乎](https://www.zhihu.com/)

---

## SGD
### 梯度下降
- stochastic gradient descent
- batch gradient descent
- mini-batch gradient descent

现在SGD通常指小批量梯度下降，每一次迭代计算mini-batch的梯度，然后对参数进行更新。

#### 缺点：
1. 选择合适的learning rate较难，对所有参数更新使用的是相同的学习率，因此无法针对稀疏特征和常出现的特征分别进行不同速率的更新。
2. SGD容易收敛到局部最优，并且在某些情况下可能困在鞍点

### **Momentum 项**是模拟物理里动量的概念，积累之前的动量来替代真正的梯度
#### 特点：
1. 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的Momentum可以进行加速
2. 下降中后期时，在局部最小值来回震荡的时候，Momentum使得更新幅度增大，跳出陷阱
3. 在梯度改变方向的时候，Momentum能够减少更新

简言之，Momentum能够在相关方向加速SGD，抑制振荡，从而加快收敛

### **Nesterov 项**在梯度更新时做一个校正，避免前进太快，同时提高灵敏度


加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正

>总而言之，momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法

---

## Adagrad

Adagrad其实是对学习率进行了一个约束

![Adagrad 1](https://www.zhihu.com/equation?tex=n_t%3Dn_%7Bt-1%7D%2Bg_t%5E2 "Adagrad 1")


![Adagrad 2](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7Bn_t%2B%5Cepsilon%7D%7D%2Ag_t "Adagrad 2")

n_t 是由g_t递推形成的约束项，$\varepsilon$ 用于保证分母非0 $\eta$ 是学习率

### 特点
- 前期g_t较小，能够放大梯度
- 后期g_t较大，能够约束梯度
- 适合处理稀疏梯度

### 缺点
- 仍要人工设置一个全局学习率
- 学习率设置过大会导致约束项的作用敏感，对梯度调节过大
- 中后期约束项不断积累会使梯度趋于0，提早结束训练


---

## Adadelta
Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值

![Adadelta 1](https://www.zhihu.com/equation?tex=n_t%3D%5Cnu%2An_%7Bt-1%7D%2B%281-%5Cnu%29%2Ag_t%5E2 "Adadelta 1")

![Adadelta 2](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D+%3D+-%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7Bn_t%2B%5Cepsilon%7D%7D%2Ag_t "Adadelta 2")

### 特点
- 训练初中期，加速效果很快
- 训练后期，反复在局部最小值附近抖动

---

## RMSprop
RMS(ROOT MEAN SQUARE 均方根)

RMSprop可以算作Adadelta的一个特例： 

### 特点：

- 其实RMSprop依然依赖于全局学习率
- RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
- 适合处理非平稳目标，对于RNN效果很好 

---

## [Adam](https://arxiv.org/abs/1412.6980)

Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。

![](https://www.zhihu.com/equation?tex=m_t%3D%5Cmu%2Am_%7Bt-1%7D%2B%281-%5Cmu%29%2Ag_t)

![](https://www.zhihu.com/equation?tex=n_t%3D%5Cnu%2An_%7Bt-1%7D%2B%281-%5Cnu%29%2Ag_t%5E2)

![](https://www.zhihu.com/equation?tex=%5Chat%7Bm_t%7D%3D%5Cfrac%7Bm_t%7D%7B1-%5Cmu%5Et%7D)

![](https://www.zhihu.com/equation?tex=%5Chat%7Bn_t%7D%3D%5Cfrac%7Bn_t%7D%7B1-%5Cnu%5Et%7D)

![](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Cfrac%7B%5Chat%7Bm_t%7D%7D%7B%5Csqrt%7B%5Chat%7Bn_t%7D%7D%2B%5Cepsilon%7D%2A%5Ceta)

m_t,n_t 分别是对梯度的一阶矩估计和二阶矩估计
m_t_hat,n_t_hat 分别是对m_t,n_t 的校正，可以近似为对期望的无偏估计

直接对梯度的矩估计对内存没有额外的要求，而且可以根据梯度进行动态调整

![](https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Chat%7Bm_t%7D%7D%7B%5Csqrt%7B%5Chat%7Bn_t%7D%7D%2B%5Cepsilon%7D) 对学习率形成动态约束，而且有明确的范围

Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。


### 特点

- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
- 对内存需求较小
- 为不同的参数计算不同的自适应学习率
- 适用于大多非凸优化 适用于大数据集和高维空间

---

## Adamax

Adamax是Adam的一种变体，Adamax学习率的边界范围更简单。

![](https://www.zhihu.com/equation?tex=n_t%3Dmax%28%5Cnu%2An_%7Bt-1%7D%2C%7Cg_t%7C%29)

![](https://www.zhihu.com/equation?tex=%5CDelta%7Bx%7D%3D-%5Cfrac%7B%5Chat%7Bm_t%7D%7D%7Bn_t%2B%5Cepsilon%7D%2A%5Ceta)

---

## Nadam
Nadam类似于带有Nesterov动量项的Adam。

![](https://www.zhihu.com/equation?tex=%5Chat%7Bg_t%7D%3D%5Cfrac%7Bg_t%7D%7B1-%5CPi_%7Bi%3D1%7D%5Et%5Cmu_i%7D)

![](https://www.zhihu.com/equation?tex=m_t%3D%5Cmu_t%2Am_%7Bt-1%7D%2B%281-%5Cmu_t%29%2Ag_t)

![](https://www.zhihu.com/equation?tex=%5Chat%7Bm_t%7D%3D%5Cfrac%7Bm_t%7D%7B1-%5CPi_%7Bi%3D1%7D%5E%7Bt%2B1%7D%5Cmu_i%7D)

![](https://www.zhihu.com/equation?tex=n_t%3D%5Cnu%2An_%7Bt-1%7D%2B%281-%5Cnu%29%2Ag_t%5E2)

![](https://www.zhihu.com/equation?tex=%5Cbar%7Bm_t%7D%3D%281-%5Cmu_t%29%2A%5Chat%7Bg_t%7D%2B%5Cmu_%7Bt%2B1%7D%2A%5Chat%7Bm_t%7D)

![](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Ceta%2A%5Cfrac%7B%5Cbar%7Bm_t%7D%7D%7B%5Csqrt%7B%5Chat%7Bn_t%7D%7D%2B%5Cepsilon%7D)


可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。 



# 小结
- 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
- SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
- 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法
- Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多
- 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果











