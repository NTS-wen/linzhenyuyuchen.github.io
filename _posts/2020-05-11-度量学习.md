---
layout:     post
title:      度量学习
subtitle:   度量学习 Loss
date:       2020-05-11
author:     LZY
header-img: img/xiaohuangren.jpg
catalog: true
tags:
    - Docker
---

# 度量学习

度量学习（Metric Learning）是一种空间映射的方法，其能够学习到一种特征（Embedding）空间，在此空间中，所有的数据都被转换成一个特征向量，并且相似样本的特征向量之间距离小，不相似样本的特征向量之间距离大，从而对数据进行区分。

在深度学习中，很多度量学习的方法都是使用成对成对的样本进行loss计算的，这类方法被称为 pair-based deep metric learning。例如，在训练模型的过程，我们随意的选取两个样本，使用模型提取特征，并计算他们特征之间的距离。 如果这两个样本属于同一个类别，那我们希望他们之间的距离应该尽量的小，甚至为0；如果这两个样本属于不同的类别，那我们希望他们之间的距离应该尽量的大，甚至是无穷大。正是根据这一原则，衍生出了许多不同类型的pair-based loss，使用这些loss对样本对之间的距离进行计算，并根据生成的loss使用各种优化方法对模型进行更新。

## Contrastive loss

模型对样本提取的特征，如果是正样本对，L期望二者的距离尽可能少，对于负样本对，L则期望距离尽可能大，这里可以设置一个阈值使得负样本对的距离超过Threshold就说明模型已经效果不错

Contrastive loss = Pos_loss + max(Threshold,Neg_loss)

## Triplet loss

Triplet Loss的思想是让负样本对之间的距离大于正样本对之间的距离，在训练过的过程中同时选取一对正样本对和负样本对，且正负样本对中有一个样本是相同的。

以前面的狗、狼、猫数据为例，首先随机选取一个样本，此样本称之为anchor样本，假设此样本类别为狗，然后选取一个与anchor样本同类别的样本（另一个狗狗），称之为positive，并让其与anchor样本组成一个正样本对（anchor-positive）；再选取一个与anchor不同类别的样本（猫），称之为negative，让其与anchor样本组成一个负样本对（anchor-negative）。这样一共选取了三个样本，即triplet。

Triplet loss = max(Threshold,Neg_loss - Pos_loss)

## Triplet center loss

Triplet Loss是让正样本对之间的距离小于负样本对之间的距离，并且存在一定的margin。因此triplet样本的选取至关重要，如果选取的triplet对没啥难度，很容就能进行区分，那大部分的时间生成的loss都为0，模型不更新，而如果使用hard mining的方法对难例进行挖掘，又会导致模型对噪声极为敏感。

Triplet Center loss的思想非常简单，原来的Triplet是计算anchor到正负样本之间的距离，现在Triplet Center是计算anchor到正负样本所在类别的中心的距离。类别中心就是该类别所有样本embedding向量的中心。

## Quadruplet loss

Quadruplet loss由两部分组成：

一部分就是正常的triplet loss，这部分loss能够让模型区分出正样本对和负样本对之间的相对距离。

另一部分是正样本对和其他任意负样本对之前的相对距离。这一部分约束可以理解成最小的类间距离都要大于类内距离，不管这些样本对是否有同样的anchor。
