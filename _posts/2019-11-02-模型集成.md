---
layout:     post
title:      模型集成
subtitle:   Bagging Boosting Stacking
date:       2019-11-02
author:     LZY
header-img: img/whatisnext.jpg
catalog: true
tags:
    - 模型集成
---

[Reference](https://www.jiqizhixin.com/articles/2019-05-15-15)

# 集成方法

## 集成学习

集成学习是一种机器学习范式。在集成学习中，我们会训练多个模型（通常称为「弱学习器」）解决相同的问题，并将它们结合起来以获得更好的结果。最重要的假设是：当弱模型被正确组合时，我们可以得到更精确和/或更鲁棒的模型。

在集成学习理论中，我们将弱学习器（或基础模型）称为「模型」，这些模型可用作设计更复杂模型的构件。在大多数情况下，这些基本模型本身的性能并不是非常好，这要么是因为它们具有较高的偏置（例如，低自由度模型），要么是因为他们的方差太大导致鲁棒性不强（例如，高自由度模型）。

集成方法的思想是通过将这些弱学习器的偏置和/或方差结合起来，从而创建一个「强学习器」（或「集成模型」），从而获得更好的性能。

## 组合弱学习器

为了建立一个集成学习方法，我们首先要选择待聚合的基础模型。在大多数情况下（包括在众所周知的 bagging 和 boosting 方法中），我们会使用单一的基础学习算法，这样一来我们就有了以不同方式训练的`同质弱学习器`。

然而，也有一些方法使用不同种类的基础学习算法：将一些`异质弱学习器`组合成`异质集成模型`

很重要的一点是：我们对弱学习器的选择应该和我们聚合这些模型的方式相一致。

如果我们选择具有低偏置高方差的基础模型，我们应该使用一种倾向于减小方差的聚合方法；而如果我们选择具有低方差高偏置的基础模型，我们应该使用一种倾向于减小偏置的聚合方法。

## 组合弱学习器的元算法

### 自助聚合（bagging）

>该方法通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。

这种统计技术先随机抽取出作为替代的 B 个观测值，然后根据一个规模为 N 的初始数据集生成大小为 B 的样本（称为自助样本）。

![](/img/640.png)

在某些假设条件下，这些样本具有非常好的统计特性：在一级近似中，它们可以被视为是直接从真实的底层（并且往往是未知的）数据分布中抽取出来的，并且彼此之间相互独立。因此，它们被认为是真实数据分布的代表性和独立样本（几乎是独立同分布的样本）。

为了使这种近似成立，必须验证两个方面的假设。

首先初始数据集的大小 N 应该足够大，以捕获底层分布的大部分复杂性。这样，从数据集中抽样就是从真实分布中抽样的良好近似（代表性）。

其次，与自助样本的大小 B 相比，数据集的规模 N 应该足够大，这样样本之间就不会有太大的相关性（独立性）。注意，接下来我可能还会提到自助样本的这些特性（代表性和独立性），但读者应该始终牢记：「这只是一种近似」。 

举例而言，自助样本通常用于评估统计估计量的方差或置信区间。根据定义，统计估计量是某些观测值的函数。因此，随机变量的方差是根据这些观测值计算得到的。

为了评估这种估计量的方差，我们需要对从感兴趣分布中抽取出来的几个独立样本进行估计。在大多数情况下，相较于实际可用的数据量来说，考虑真正独立的样本所需要的数据量可能太大了。

然而，我们可以使用自助法生成一些自助样本，它们可被视为「最具代表性」以及「最具独立性」（几乎是独立同分布的样本）的样本。这些自助样本使我们可以通过估计每个样本的值，近似得到估计量的方差。

![](/img/641.png)

### 提升法（boosting）

>该方法通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并按照某种确定性的策略将它们组合起来。

在「顺序化的方法中」，组合起来的不同弱模型之间不再相互独立地拟合。其思想是「迭代地」拟合模型，使模型在给定步骤上的训练依赖于之前的步骤上拟合的模型。「Boosting」是这些方法中最著名的一种，它生成的集成模型通常比组成该模型的弱学习器偏置更小。

Boosting 方法和bagging 方法的工作思路是一样的：我们构建一系列模型，将它们聚合起来得到一个性能更好的强学习器。然而，与重点在于减小方差的 bagging 不同，boosting 着眼于以一种适应性很强的方式顺序拟合多个弱学习器：序列中每个模型在拟合的过程中，会更加重视那些序列中之前的模型处理地很糟糕的观测数据。

直观地说，每个模型都把注意力集中在目前最难拟合的观测数据上。这样一来，在这个过程的最后，我们就获得了一个具有较低偏置的强学习器（我们会注意到，boosting 也有减小方差的效果）。和 bagging 一样，Boosting 也可以用于回归和分类问题。

由于其重点在于减小偏置，用于 boosting  的基础模型通常是那些低方差高偏置的模型。例如，如果想要使用树作为基础模型，我们将主要选择只有少许几层的较浅决策树。

而选择低方差高偏置模型作为 boosting 弱学习器的另一个重要原因是：这些模型拟合的计算开销较低（参数化时自由度较低）。

实际上，由于拟合不同模型的计算无法并行处理（与 bagging 不同），顺序地拟合若干复杂模型会导致计算开销变得非常高。

一旦选定了弱学习器，我们仍需要定义它们的拟合方式（在拟合当前模型时，要考虑之前模型的哪些信息？）和聚合方式（如何将当前的模型聚合到之前的模型中？）

两个重要的 boosting 算法：`自适应提升（adaboost）`和`梯度提升（gradient boosting）`

简而言之，这两种元算法在顺序化的过程中创建和聚合弱学习器的方式存在差异。自适应增强算法会更新附加给每个训练数据集中观测数据的权重，而梯度提升算法则会更新这些观测数据的值。这里产生差异的主要原因是：两种算法解决优化问题（寻找最佳模型——弱学习器的加权和）的方式不同。

![](/img/642.png)

Boosting 会迭代地拟合一个弱学习器，将其聚合到集成模型中，并「更新」训练数据集，从而在拟合下一个基础模型时更好地考虑当前集成模型的优缺点。


### 堆叠法（stacking）

>该方法通常考虑的是异质弱学习器，并行地学习它们，并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。

Stacking 与 bagging 和 boosting 主要存在两方面的差异。首先，Stacking 通常考虑的是异质弱学习器（不同的学习算法被组合在一起），而bagging 和 boosting 主要考虑的是同质弱学习器。其次，stacking 学习用元模型组合基础模型，而bagging 和 boosting 则根据确定性算法组合弱学习器。

正如上文已经提到的，stacking 的概念是学习几个不同的弱学习器，并通过训练一个元模型来组合它们，然后基于这些弱模型返回的多个预测结果输出最终的预测结果。

因此，为了构建 stacking 模型，我们需要定义两个东西：`想要拟合的 L 个学习器`以及`组合它们的元模型`。

例如，对于分类问题来说，我们可以选择 KNN 分类器、logistic 回归和SVM 作为弱学习器，并决定学习神经网络作为元模型。然后，神经网络将会把三个弱学习器的输出作为输入，并返回基于该输入的最终预测。

所以，假设我们想要拟合由 L 个弱学习器组成的 stacking 集成模型。我们必须遵循以下步骤：

1. 将训练数据分为两组

2. 选择 L 个弱学习器，用它们拟合第一组数据

3. 使 L 个学习器中的每个学习器对第二组数据中的观测数据进行预测

4. 在第二组数据上拟合`元模型`，使用弱学习器做出的预测作为输入

在前面的步骤中，我们将数据集一分为二，因为对用于训练弱学习器的数据的预测与元模型的训练不相关。因此，将数据集分成两部分的一个明显缺点是，我们只有一半的数据用于训练基础模型，另一半数据用于训练元模型。

为了克服这种限制，我们可以使用某种「k-折交叉训练」方法（类似于 k-折交叉验证中的做法）。这样所有的观测数据都可以用来训练元模型：对于任意的观测数据，弱学习器的预测都是通过在 k-1 折数据（不包含已考虑的观测数据）上训练这些弱学习器的实例来完成的。

换句话说，它会在 k-1 折数据上进行训练，从而对剩下的一折数据进行预测。迭代地重复这个过程，就可以得到对任何一折观测数据的预测结果。

这样一来，我们就可以为数据集中的每个观测数据生成相关的预测，然后使用所有这些预测结果**训练元模型**。

![](/img/643.png)

